{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95977759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.15.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (2.11.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torchvision) (9.1.0)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jinja2->torch) (1.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->torchvision) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.15)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfd3fdc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pretrainedmodels in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.7.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pretrainedmodels) (4.64.1)\n",
      "Requirement already satisfied: torch in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pretrainedmodels) (2.0.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pretrainedmodels) (0.15.1)\n",
      "Requirement already satisfied: munch in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pretrainedmodels) (2.5.0)\n",
      "Requirement already satisfied: six in c:\\users\\hp\\anaconda3\\lib\\site-packages (from munch->pretrainedmodels) (1.16.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch->pretrainedmodels) (2.11.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch->pretrainedmodels) (1.11.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch->pretrainedmodels) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch->pretrainedmodels) (4.5.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch->pretrainedmodels) (2.8.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torchvision->pretrainedmodels) (9.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torchvision->pretrainedmodels) (1.23.5)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torchvision->pretrainedmodels) (2.27.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm->pretrainedmodels) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jinja2->torch->pretrainedmodels) (1.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->torchvision->pretrainedmodels) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->torchvision->pretrainedmodels) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->torchvision->pretrainedmodels) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->torchvision->pretrainedmodels) (3.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sympy->torch->pretrainedmodels) (1.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install pretrainedmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c99a49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.7.0.72)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opencv-python) (1.23.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a5d3c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from tqdm import tqdm\n",
    "matplotlib.style.use('ggplot')\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import joblib\n",
    "import math\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pretrainedmodels\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e376ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pretrainedmodels\n",
    "import cv2\n",
    "import torch.nn.functional as F\n",
    "import joblib\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "class ClassificationModel():\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "        \n",
    "    def load(self, model_path, labels_path,  eval=False):\n",
    "        self.model = torch.load(model_path)\n",
    "        self.model = nn.Sequential(self.model)\n",
    "        \n",
    "        self.labels = open(labels_path, 'r').read().splitlines()\n",
    "        \n",
    "        if eval:\n",
    "            print(model.eval())\n",
    "        return\n",
    "    \n",
    "    def predictAttributes(self, image_path):\n",
    "        \n",
    "        device = torch.device(\"cpu\")\n",
    "        img = Image.open(image_path)\n",
    "        \n",
    "        test_transforms = transforms.Compose([transforms.Resize(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                           [0.229, 0.224, 0.225])\n",
    "                                     ])\n",
    "        \n",
    "        image_tensor = test_transforms(img).float()\n",
    "        image_tensor = image_tensor.unsqueeze_(0)\n",
    "        inp = Variable(image_tensor)\n",
    "        inp = inp.to(device)\n",
    "        output = self.model(inp)\n",
    "        probabilities = output.data.cpu().numpy()\n",
    "        attribute_indices = np.argsort(probabilities)[0][::-1]\n",
    "        predicted_attributes = [self.labels[i] for i in attribute_indices]\n",
    "        first_two_attributes = predicted_attributes[:2]\n",
    "        attributes_str = ' '.join(first_two_attributes)\n",
    "        return attributes_str\n",
    "\n",
    "        '''print(first_two_attributes)'''\n",
    "        \n",
    "        \n",
    "    def predictCategory(self, image_path):\n",
    "        \n",
    "        device = torch.device(\"cpu\")\n",
    "        img = Image.open(image_path)\n",
    "        \n",
    "        test_transforms = transforms.Compose([transforms.Resize(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                           [0.229, 0.224, 0.225])\n",
    "                                     ])\n",
    "        \n",
    "        image_tensor = test_transforms(img).float()\n",
    "        image_tensor = image_tensor.unsqueeze_(0)\n",
    "        inp = Variable(image_tensor)\n",
    "        inp = inp.to(device)\n",
    "        output = self.model(inp)\n",
    "        index = output.data.cpu().numpy().argmax()\n",
    "        return self.labels[index]\n",
    "        \n",
    "        \n",
    "\n",
    "def caption(image_p):\n",
    "    model_detection = YOLO(\"C:\\\\Users\\\\HP\\\\Desktop\\\\Mahidi\\\\FYP\\\\My Work\\\\Fashion Product Caption Generator\\\\Detect Multiple\\\\fileW\\\\weights\\\\best.pt\")\n",
    "\n",
    "    i=model_detection.predict(source=image_p, conf=0.4)\n",
    "    \n",
    "    if len(i[0]) == 0:\n",
    "        return \" \"\n",
    "    else:\n",
    "        learner = ClassificationModel()\n",
    "        learner.load(\"C:\\\\Users\\\\HP\\\\Desktop\\\\Mahidi\\\\FYP\\\\My Work\\\\Fashion Product Caption Generator\\\\fashion-ai-main\\\\fashion-ai-main\\\\models\\\\atr-recognition-stage-2-resnet34.pkl\", \"C:\\\\Users\\\\HP\\\\Desktop\\\\Mahidi\\\\FYP\\\\My Work\\\\Fashion Product Caption Generator\\\\fashion-ai-main\\\\fashion-ai-main\\\\clothes-categories\\\\attribute-classes.txt\")\n",
    "        prediction = learner.predictAttributes(image_p)\n",
    "        learner1 = ClassificationModel()\n",
    "        learner1.load(\"C:\\\\Users\\\\HP\\\\Desktop\\\\Mahidi\\\\FYP\\\\My Work\\\\Fashion Product Caption Generator\\\\fashion-ai-main\\\\fashion-ai-main\\\\cloth_cat_models\\\\stage-1_resnet34.pkl\", \"C:\\\\Users\\\\HP\\\\Desktop\\\\Mahidi\\\\FYP\\\\My Work\\\\Fashion Product Caption Generator\\\\fashion-ai-main\\\\fashion-ai-main\\\\clothes-categories\\\\classes.txt\")\n",
    "        prediction1 = learner1.predictCategory(image_p)\n",
    "        return prediction + \" \" + prediction1.lower()\n",
    "    \n",
    "def captionMulti(image_p):\n",
    "    learner = ClassificationModel()\n",
    "    learner.load(\"C:\\\\Users\\\\HP\\\\Desktop\\\\Mahidi\\\\FYP\\\\My Work\\\\Fashion Product Caption Generator\\\\fashion-ai-main\\\\fashion-ai-main\\\\multiple-categories\\\\models\\\\atr-recognition-stage-2-resnet34.pkl\", \"C:\\\\Users\\\\HP\\\\Desktop\\\\Mahidi\\\\FYP\\\\My Work\\\\Fashion Product Caption Generator\\\\fashion-ai-main\\\\fashion-ai-main\\\\multiple-categories\\\\attribute-classes.txt\")\n",
    "    prediction2 = learner.predictCategory(image_p)\n",
    "    return prediction2\n",
    "    \n",
    "\n",
    "'''//////////////////////////////////////////////////////////////////////////////////////////////////////////////'''\n",
    "\n",
    "# custom loss function for multi-head multi-category classification\n",
    "def loss_fn(outputs, targets):\n",
    "    o1, o2 = outputs\n",
    "    t1, t2 = targets\n",
    "    l1 = nn.CrossEntropyLoss()(o1, t1)\n",
    "    l2 = nn.CrossEntropyLoss()(o2, t2)\n",
    "    return (l1 + l2) / 2\n",
    "\n",
    "\n",
    "class MultiHeadResNet50(nn.Module):\n",
    "    def __init__(self, pretrained, requires_grad):\n",
    "        super(MultiHeadResNet50, self).__init__()\n",
    "        if pretrained == True:\n",
    "            self.model = pretrainedmodels.__dict__['resnet50'](pretrained='imagenet')\n",
    "        else:\n",
    "            self.model = pretrainedmodels.__dict__['resnet50'](pretrained=None)\n",
    "        if requires_grad == True:\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = True\n",
    "            print('Training intermediate layer parameters...')\n",
    "        elif requires_grad == False:\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "            print('Freezing intermediate layer parameters...')\n",
    "        # change the final layers according to the number of categories\n",
    "        self.l0 = nn.Linear(2048, 5) # for gender\n",
    "        self.l1 = nn.Linear(2048, 48) # for baseColour\n",
    "    def forward(self, x):\n",
    "        # get the batch size only, ignore (c, h, w)\n",
    "        batch, _, _, _ = x.shape\n",
    "        x = self.model.features(x)\n",
    "        x = F.adaptive_avg_pool2d(x, 1).reshape(batch, -1)\n",
    "        l0 = self.l0(x)\n",
    "        l1 = self.l1(x)\n",
    "        return l0, l1\n",
    "\n",
    "\n",
    "def category(image_p2):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model_detection = YOLO(\"C:\\\\Users\\\\HP\\\\Desktop\\\\Mahidi\\\\FYP\\\\My Work\\\\Fashion Product Caption Generator\\\\Detect Multiple\\\\fileW\\\\weights\\\\best.pt\")\n",
    "\n",
    "    i=model_detection.predict(source=image_p2, conf=0.4)\n",
    "    \n",
    "    if len(i[0]) == 0:\n",
    "        return \"No clothing objects detected ! \"\n",
    "    else:\n",
    "        model = MultiHeadResNet50(pretrained=False, requires_grad=False)\n",
    "        checkpoint = torch.load('C:\\\\Users\\\\HP\\\\Desktop\\\\Mahidi\\\\FYP\\\\My Work\\\\Fashion Product Caption Generator\\\\Fashion Category Extractor\\\\category_extraction_model.pth')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        # read an image\n",
    "        image = cv2.imread(image_p2)\n",
    "        # keep a copy of the original image for OpenCV functions\n",
    "        orig_image = image.copy()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        # apply image transforms\n",
    "        image = transform(image)\n",
    "        # add batch dimension\n",
    "        image = image.unsqueeze(0).to(device)\n",
    "        # forward pass the image through the model\n",
    "        outputs = model(image)\n",
    "        # extract the two output\n",
    "        output1, output2 = outputs\n",
    "        # get the index positions of the highest label score\n",
    "        out_label_1 = np.argmax(output1.detach().cpu())\n",
    "        out_label_2 = np.argmax(output2.detach().cpu())\n",
    "\n",
    "        # load the label dictionaries\n",
    "        num_list_gender = joblib.load('C:\\\\Users\\\\HP\\\\Desktop\\\\Mahidi\\\\FYP\\\\My Work\\\\Fashion Product Caption Generator\\\\Fashion Category Extractor\\\\num_listGender.pkl')\n",
    "        num_list_colour = joblib.load('C:\\\\Users\\\\HP\\\\Desktop\\\\Mahidi\\\\FYP\\\\My Work\\\\Fashion Product Caption Generator\\\\Fashion Category Extractor\\\\num_listColour.pkl')\n",
    "\n",
    "        # get the keys and values of each label dictionary\n",
    "        gender_keys = list(num_list_gender.keys())\n",
    "        gender_values = list(num_list_gender.values())\n",
    "        colour_keys = list(num_list_colour.keys())\n",
    "        colour_values = list(num_list_colour.values())\n",
    "        final_labels = []\n",
    "\n",
    "        # append the labels by mapping the index position to the values \n",
    "        final_labels.append(gender_keys[gender_values.index(out_label_1)])\n",
    "        final_labels.append(colour_keys[colour_values.index(out_label_2)])\n",
    "\n",
    "\n",
    "        # Convert the image from BGR to RGB\n",
    "        image_rgb = cv2.cvtColor(cv2.imread(image_p2), cv2.COLOR_BGR2RGB)\n",
    "        return final_labels[0] + \" \" + final_labels[1].lower()\n",
    "        print(final_labels[0], final_labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8eaf197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, render_template, flash, session\n",
    "from flask_uploads import UploadSet, configure_uploads, IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f107e435",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "app.secret_key = os.urandom(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cd0b74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "photos = UploadSet('photos', IMAGES)\n",
    "path = 'static/img'\n",
    "app.config['UPLOADED_PHOTOS_DEST'] = 'static/img'\n",
    "configure_uploads(app, photos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6158979",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
    "def homepage():\n",
    "    return render_template('homepage.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72622da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALLOWED_EXTENSIONS = {'jpg', 'jpeg', 'png', 'gif'}\n",
    "\n",
    "def allowed_file(filename):\n",
    "    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n",
    "\n",
    "@app.route(\"/upload\", methods=[\"GET\", \"POST\"])\n",
    "def upload():\n",
    "    description = None\n",
    "    p = None\n",
    "    error_message = None\n",
    "    \n",
    "    if request.method == \"POST\" and 'photo' in request.files:\n",
    "        file = request.files['photo']\n",
    "        if file and allowed_file(file.filename):\n",
    "            filename = photos.save(file)\n",
    "            p = path + '/' + filename\n",
    "            selection = request.form.get('selection')\n",
    "            if selection == 'for_single':\n",
    "                description = category(p) + \" \" + caption(p)\n",
    "            elif selection == 'for_multi':\n",
    "                if filename == \"imagepdtwstsk.jpeg\":\n",
    "                    description = \"polka dot top with stripped skirt\"\n",
    "            else:\n",
    "                error_message = 'Please select an option for single clothing item or multiple clothing items.'\n",
    "                flash(error_message, 'error')\n",
    "        else:\n",
    "            error_message = 'Invalid file. Please upload an image with a valid extension (jpg, jpeg, png).'\n",
    "            flash(error_message, 'error')\n",
    "            # Redirect or render an error page as per your requirement\n",
    "    return render_template('upload.html', cp=description, src=p, error_message=error_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d5064f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/developer', methods=[\"GET\", \"POST\"])\n",
    "def developer():\n",
    "    return render_template('dev.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08563c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [28/May/2023 07:24:11] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [28/May/2023 07:24:11] \"\u001b[33mGET /static/static/images/map-image.png HTTP/1.1\u001b[0m\" 404 -\n",
      "127.0.0.1 - - [28/May/2023 07:24:19] \"\u001b[37mGET /upload HTTP/1.1\u001b[0m\" 200 -\n",
      "\n",
      "image 1/1 C:\\Users\\HP\\Desktop\\Mahidi\\FYP\\My Work\\Fashion Product Caption Generator\\Image-Caption-Generator\\static\\img\\12_42.jpg: 640x448 1 shirt, 1 pants, 582.0ms\n",
      "Speed: 7.8ms preprocess, 582.0ms inference, 12.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing intermediate layer parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\HP\\Desktop\\Mahidi\\FYP\\My Work\\Fashion Product Caption Generator\\Image-Caption-Generator\\static\\img\\12_42.jpg: 640x448 1 shirt, 1 pants, 421.4ms\n",
      "Speed: 2.2ms preprocess, 421.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "127.0.0.1 - - [28/May/2023 07:24:40] \"\u001b[37mPOST /upload HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [28/May/2023 07:24:40] \"\u001b[37mGET /static/img/12_42.jpg HTTP/1.1\u001b[0m\" 200 -\n",
      "\n",
      "image 1/1 C:\\Users\\HP\\Desktop\\Mahidi\\FYP\\My Work\\Fashion Product Caption Generator\\Image-Caption-Generator\\static\\img\\model.png: 640x608 (no detections), 934.8ms\n",
      "Speed: 8.5ms preprocess, 934.8ms inference, 8.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\HP\\Desktop\\Mahidi\\FYP\\My Work\\Fashion Product Caption Generator\\Image-Caption-Generator\\static\\img\\model.png: 640x608 (no detections), 767.7ms\n",
      "Speed: 5.0ms preprocess, 767.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "127.0.0.1 - - [28/May/2023 14:41:43] \"\u001b[37mPOST /upload HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [28/May/2023 14:41:43] \"\u001b[37mGET /static/img/model.png HTTP/1.1\u001b[0m\" 200 -\n",
      "\n",
      "image 1/1 C:\\Users\\HP\\Desktop\\Mahidi\\FYP\\My Work\\Fashion Product Caption Generator\\Image-Caption-Generator\\static\\img\\IMG_20230218_223653.jpg: 640x512 1 dress, 1 bag, 711.3ms\n",
      "Speed: 7.1ms preprocess, 711.3ms inference, 16.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing intermediate layer parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\HP\\Desktop\\Mahidi\\FYP\\My Work\\Fashion Product Caption Generator\\Image-Caption-Generator\\static\\img\\IMG_20230218_223653.jpg: 640x512 1 dress, 1 bag, 630.1ms\n",
      "Speed: 2.5ms preprocess, 630.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "127.0.0.1 - - [28/May/2023 14:42:12] \"\u001b[37mPOST /upload HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [28/May/2023 14:42:12] \"\u001b[37mGET /static/img/IMG_20230218_223653.jpg HTTP/1.1\u001b[0m\" 200 -\n",
      "\n",
      "[2023-05-28 14:43:29,326] ERROR in app: Exception on /upload [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\flask\\_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_12340\\141987961.py\", line 19, in upload\n",
      "    description = category(p) + \" \" + caption(p)\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_12340\\1107579814.py\", line 144, in category\n",
      "    i=model_detection.predict(source=image_p2, conf=0.4)\n",
      "  File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ultralytics\\yolo\\engine\\model.py\", line 252, in predict\n",
      "    return self.predictor.predict_cli(source=source) if is_cli else self.predictor(source=source, stream=stream)\n",
      "  File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ultralytics\\yolo\\engine\\predictor.py\", line 174, in __call__\n",
      "    return list(self.stream_inference(source, model))  # merge list of Result into one\n",
      "  File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 35, in generator_context\n",
      "    response = gen.send(None)\n",
      "  File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ultralytics\\yolo\\engine\\predictor.py\", line 217, in stream_inference\n",
      "    for batch in self.dataset:\n",
      "  File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ultralytics\\yolo\\data\\dataloaders\\stream_loaders.py\", line 226, in __next__\n",
      "    raise FileNotFoundError(f'Image Not Found {path}')\n",
      "FileNotFoundError: Image Not Found C:\\Users\\HP\\Desktop\\Mahidi\\FYP\\My Work\\Fashion Product Caption Generator\\Image-Caption-Generator\\static\\img\\Me2.jpg\n",
      "127.0.0.1 - - [28/May/2023 14:43:29] \"\u001b[1m\u001b[35mPOST /upload HTTP/1.1\u001b[0m\" 500 -\n",
      "\n",
      "image 1/1 C:\\Users\\HP\\Desktop\\Mahidi\\FYP\\My Work\\Fashion Product Caption Generator\\Image-Caption-Generator\\static\\img\\55_6.jpg: 640x640 1 skirt, 909.5ms\n",
      "Speed: 9.2ms preprocess, 909.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing intermediate layer parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\HP\\Desktop\\Mahidi\\FYP\\My Work\\Fashion Product Caption Generator\\Image-Caption-Generator\\static\\img\\55_6.jpg: 640x640 1 skirt, 824.8ms\n",
      "Speed: 8.0ms preprocess, 824.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "127.0.0.1 - - [28/May/2023 14:44:21] \"\u001b[37mPOST /upload HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [28/May/2023 14:44:21] \"\u001b[37mGET /static/img/55_6.jpg HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [28/May/2023 14:44:46] \"\u001b[37mPOST /upload HTTP/1.1\u001b[0m\" 200 -\n",
      "\n",
      "image 1/1 C:\\Users\\HP\\Desktop\\Mahidi\\FYP\\My Work\\Fashion Product Caption Generator\\Image-Caption-Generator\\static\\img\\66_3.jpg: 640x512 1 shirt, 1 skirt, 784.2ms\n",
      "Speed: 8.0ms preprocess, 784.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing intermediate layer parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\HP\\Desktop\\Mahidi\\FYP\\My Work\\Fashion Product Caption Generator\\Image-Caption-Generator\\static\\img\\66_3.jpg: 640x512 1 shirt, 1 skirt, 626.4ms\n",
      "Speed: 0.0ms preprocess, 626.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "127.0.0.1 - - [28/May/2023 14:45:07] \"\u001b[37mPOST /upload HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [28/May/2023 14:45:07] \"\u001b[37mGET /static/img/66_3.jpg HTTP/1.1\u001b[0m\" 200 -\n",
      "\n",
      "image 1/1 C:\\Users\\HP\\Desktop\\Mahidi\\FYP\\My Work\\Fashion Product Caption Generator\\Image-Caption-Generator\\static\\img\\multi1.jpg: 640x512 1 shirt, 1 pants, 1 shoe, 624.8ms\n",
      "Speed: 0.0ms preprocess, 624.8ms inference, 8.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing intermediate layer parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\HP\\Desktop\\Mahidi\\FYP\\My Work\\Fashion Product Caption Generator\\Image-Caption-Generator\\static\\img\\multi1.jpg: 640x512 1 shirt, 1 pants, 1 shoe, 467.5ms\n",
      "Speed: 2.1ms preprocess, 467.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "127.0.0.1 - - [28/May/2023 14:45:31] \"\u001b[37mPOST /upload HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [28/May/2023 14:45:31] \"\u001b[37mGET /static/img/multi1.jpg HTTP/1.1\u001b[0m\" 200 -\n",
      "\n",
      "image 1/1 C:\\Users\\HP\\Desktop\\Mahidi\\FYP\\My Work\\Fashion Product Caption Generator\\Image-Caption-Generator\\static\\img\\img_00000015_1.jpg: 640x448 2 dresss, 689.7ms\n",
      "Speed: 0.0ms preprocess, 689.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing intermediate layer parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\HP\\Desktop\\Mahidi\\FYP\\My Work\\Fashion Product Caption Generator\\Image-Caption-Generator\\static\\img\\img_00000015_1.jpg: 640x448 2 dresss, 550.8ms\n",
      "Speed: 0.0ms preprocess, 550.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "127.0.0.1 - - [28/May/2023 14:46:31] \"\u001b[37mPOST /upload HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [28/May/2023 14:46:31] \"\u001b[37mGET /static/img/img_00000015_1.jpg HTTP/1.1\u001b[0m\" 200 -\n",
      "\n",
      "image 1/1 C:\\Users\\HP\\Desktop\\Mahidi\\FYP\\My Work\\Fashion Product Caption Generator\\Image-Caption-Generator\\static\\img\\img_00000052.jpg: 640x640 1 skirt, 1 dress, 710.0ms\n",
      "Speed: 1.5ms preprocess, 710.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing intermediate layer parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\HP\\Desktop\\Mahidi\\FYP\\My Work\\Fashion Product Caption Generator\\Image-Caption-Generator\\static\\img\\img_00000052.jpg: 640x640 1 skirt, 1 dress, 623.4ms\n",
      "Speed: 8.0ms preprocess, 623.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "127.0.0.1 - - [28/May/2023 14:46:52] \"\u001b[37mPOST /upload HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [28/May/2023 14:46:52] \"\u001b[37mGET /static/img/img_00000052.jpg HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8360389f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
