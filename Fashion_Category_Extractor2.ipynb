{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3490ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.15.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (2.11.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torchvision) (9.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jinja2->torch) (1.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->torchvision) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccfe2841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pretrainedmodels in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.7.4)\n",
      "Requirement already satisfied: torchvision in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pretrainedmodels) (0.15.1)\n",
      "Requirement already satisfied: torch in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pretrainedmodels) (2.0.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pretrainedmodels) (4.64.1)\n",
      "Requirement already satisfied: munch in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pretrainedmodels) (2.5.0)\n",
      "Requirement already satisfied: six in c:\\users\\hp\\anaconda3\\lib\\site-packages (from munch->pretrainedmodels) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch->pretrainedmodels) (4.5.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch->pretrainedmodels) (2.11.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch->pretrainedmodels) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch->pretrainedmodels) (2.8.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch->pretrainedmodels) (3.9.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torchvision->pretrainedmodels) (9.1.0)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torchvision->pretrainedmodels) (2.27.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torchvision->pretrainedmodels) (1.23.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm->pretrainedmodels) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jinja2->torch->pretrainedmodels) (1.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->torchvision->pretrainedmodels) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->torchvision->pretrainedmodels) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->torchvision->pretrainedmodels) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->torchvision->pretrainedmodels) (2.0.12)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sympy->torch->pretrainedmodels) (1.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install pretrainedmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62c8ec47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from tqdm import tqdm\n",
    "matplotlib.style.use('ggplot')\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import joblib\n",
    "import math\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pretrainedmodels\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4950bbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0e1a753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.7.0.72)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opencv-python) (1.23.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1368411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset\n",
    "# import torch\n",
    "# import joblib\n",
    "# import math\n",
    "# import cv2\n",
    "# import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12db0c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# custom loss function for multi-head multi-category classification\n",
    "def loss_fn(outputs, targets):\n",
    "    o1, o2 = outputs\n",
    "    t1, t2 = targets\n",
    "    l1 = nn.CrossEntropyLoss()(o1, t1)\n",
    "    l2 = nn.CrossEntropyLoss()(o2, t2)\n",
    "    return (l1 + l2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68b98adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import pretrainedmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54cfec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import torch\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader\n",
    "# from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f547a752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing intermediate layer parameters...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Men Navy Blue'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pretrainedmodels\n",
    "import cv2\n",
    "import torch.nn.functional as F\n",
    "import joblib\n",
    "\n",
    "'''MODEL_PATH = \"C:\\\\Users\\\\HP\\\\Desktop\\\\Mahidi\\\\FYP\\\\My Work\\\\Fashion Product Caption Generator\\\\fashion-ai-main\\\\fashion-ai-main\\\\models\\\\atr-recognition-stage-2-resnet34.pkl\"\n",
    "CLASSES_PATH = \"C:\\\\Users\\\\HP\\\\Desktop\\\\Mahidi\\\\FYP\\\\My Work\\\\Fashion Product Caption Generator\\\\fashion-ai-main\\\\fashion-ai-main\\\\clothes-categories\\\\attribute-classes.txt\"\n",
    "'''\n",
    "class ClassificationModel():\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "        \n",
    "    def load(self, model_path, labels_path,  eval=False):\n",
    "        self.model = torch.load(model_path)\n",
    "        self.model = nn.Sequential(self.model)\n",
    "        \n",
    "        self.labels = open(labels_path, 'r').read().splitlines()\n",
    "        \n",
    "        if eval:\n",
    "            print(model.eval())\n",
    "        return\n",
    "    \n",
    "    def predict(self, image_path):\n",
    "        \n",
    "        device = torch.device(\"cpu\")\n",
    "        img = Image.open(image_path)\n",
    "        \n",
    "        test_transforms = transforms.Compose([transforms.Resize(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                           [0.229, 0.224, 0.225])\n",
    "                                     ])\n",
    "        \n",
    "        image_tensor = test_transforms(img).float()\n",
    "        image_tensor = image_tensor.unsqueeze_(0)\n",
    "        inp = Variable(image_tensor)\n",
    "        inp = inp.to(device)\n",
    "        output = self.model(inp)\n",
    "        probabilities = output.data.cpu().numpy()\n",
    "        attribute_indices = np.argsort(probabilities)[0][::-1]\n",
    "        predicted_attributes = [self.labels[i] for i in attribute_indices]\n",
    "        first_two_attributes = predicted_attributes[:2]\n",
    "        attributes_str = ' '.join(first_two_attributes)\n",
    "        return attributes_str\n",
    "\n",
    "        '''print(first_two_attributes)'''\n",
    "        \n",
    "        \n",
    "    def predict1(self, image_path):\n",
    "        \n",
    "        device = torch.device(\"cpu\")\n",
    "        img = Image.open(image_path)\n",
    "        \n",
    "        test_transforms = transforms.Compose([transforms.Resize(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                           [0.229, 0.224, 0.225])\n",
    "                                     ])\n",
    "        \n",
    "        image_tensor = test_transforms(img).float()\n",
    "        image_tensor = image_tensor.unsqueeze_(0)\n",
    "        inp = Variable(image_tensor)\n",
    "        inp = inp.to(device)\n",
    "        output = self.model(inp)\n",
    "        index = output.data.cpu().numpy().argmax()\n",
    "        return self.labels[index]\n",
    "        \n",
    "        \n",
    "\n",
    "def caption(image_p):\n",
    "    '''\n",
    "    image_p = \"C:\\\\Users\\\\HP\\\\Desktop\\\\Mahidi\\\\FYP\\\\My Work\\\\Fashion Product Caption Generator\\\\fashion-ai-main\\\\fashion-ai-main\\\\clothes-categories\\\\img\\\\img\\\\Velveteen_Floral_Dress\\\\img_00000013.jpg\"\n",
    "    '''\n",
    "    learner = ClassificationModel()\n",
    "    learner.load(\"C:\\\\Users\\\\HP\\\\Desktop\\\\Mahidi\\\\FYP\\\\My Work\\\\Fashion Product Caption Generator\\\\fashion-ai-main\\\\fashion-ai-main\\\\models\\\\atr-recognition-stage-2-resnet34.pkl\", \"C:\\\\Users\\\\HP\\\\Desktop\\\\Mahidi\\\\FYP\\\\My Work\\\\Fashion Product Caption Generator\\\\fashion-ai-main\\\\fashion-ai-main\\\\clothes-categories\\\\attribute-classes.txt\")\n",
    "    prediction = learner.predict(image_p)\n",
    "    learner1 = ClassificationModel()\n",
    "    learner1.load(\"C:\\\\Users\\\\HP\\\\Desktop\\\\Mahidi\\\\FYP\\\\My Work\\\\Fashion Product Caption Generator\\\\fashion-ai-main\\\\fashion-ai-main\\\\cloth_cat_models\\\\stage-1_resnet34.pkl\", \"C:\\\\Users\\\\HP\\\\Desktop\\\\Mahidi\\\\FYP\\\\My Work\\\\Fashion Product Caption Generator\\\\fashion-ai-main\\\\fashion-ai-main\\\\clothes-categories\\\\classes.txt\")\n",
    "    prediction1 = learner1.predict1(image_p)\n",
    "    return prediction + \" \" + prediction1\n",
    "    '''print(prediction, prediction1)'''\n",
    "    \n",
    "'''caption()'''\n",
    "    \n",
    "'''def caption1():\n",
    "    learner = ClassificationModel()\n",
    "    learner.load(\"C:\\\\Users\\\\HP\\\\Desktop\\\\Mahidi\\\\FYP\\\\My Work\\\\Fashion Product Caption Generator\\\\fashion-ai-main\\\\fashion-ai-main\\\\\\cloth_cat_models\\\\stage-1_resnet34.pkl\", \"C:\\\\Users\\\\HP\\\\Desktop\\\\Mahidi\\\\FYP\\\\My Work\\\\Fashion Product Caption Generator\\\\fashion-ai-main\\\\fashion-ai-main\\\\clothes-categories\\\\classes.txt\")\n",
    "    prediction = learner.predict1(\"C:\\\\Users\\\\HP\\\\Desktop\\\\Mahidi\\\\FYP\\\\My Work\\\\Fashion Product Caption Generator\\\\fashion-ai-main\\\\fashion-ai-main\\\\clothes-categories\\\\img\\\\img\\\\Velveteen_Floral_Dress\\\\img_00000013.jpg\")\n",
    "    print(\"Predicted result:\", prediction)'''\n",
    "    \n",
    "\n",
    "'''//////////////////////////////////////////////////////////////////////////////////////////////////////////////'''\n",
    "class MultiHeadResNet50(nn.Module):\n",
    "    def __init__(self, pretrained, requires_grad):\n",
    "        super(MultiHeadResNet50, self).__init__()\n",
    "        if pretrained == True:\n",
    "            self.model = pretrainedmodels.__dict__['resnet50'](pretrained='imagenet')\n",
    "        else:\n",
    "            self.model = pretrainedmodels.__dict__['resnet50'](pretrained=None)\n",
    "        if requires_grad == True:\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = True\n",
    "            print('Training intermediate layer parameters...')\n",
    "        elif requires_grad == False:\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "            print('Freezing intermediate layer parameters...')\n",
    "        # change the final layers according to the number of categories\n",
    "        self.l0 = nn.Linear(2048, 5) # for gender\n",
    "        self.l1 = nn.Linear(2048, 48) # for baseColour\n",
    "    def forward(self, x):\n",
    "        # get the batch size only, ignore (c, h, w)\n",
    "        batch, _, _, _ = x.shape\n",
    "        x = self.model.features(x)\n",
    "        x = F.adaptive_avg_pool2d(x, 1).reshape(batch, -1)\n",
    "        l0 = self.l0(x)\n",
    "        l1 = self.l1(x)\n",
    "        return l0, l1\n",
    "\n",
    "\n",
    "def category(image_p2):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model = MultiHeadResNet50(pretrained=False, requires_grad=False)\n",
    "    checkpoint = torch.load('C:\\\\Users\\\\HP\\\\Desktop\\\\Mahidi\\\\FYP\\\\My Work\\\\Fashion Product Caption Generator\\\\Fashion Category Extractor\\\\category_extraction_model.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # read an image\n",
    "    image = cv2.imread(image_p2)\n",
    "    # keep a copy of the original image for OpenCV functions\n",
    "    orig_image = image.copy()\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # apply image transforms\n",
    "    image = transform(image)\n",
    "    # add batch dimension\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    # forward pass the image through the model\n",
    "    outputs = model(image)\n",
    "    # extract the two output\n",
    "    output1, output2 = outputs\n",
    "    # get the index positions of the highest label score\n",
    "    out_label_1 = np.argmax(output1.detach().cpu())\n",
    "    out_label_2 = np.argmax(output2.detach().cpu())\n",
    "\n",
    "    # load the label dictionaries\n",
    "    num_list_gender = joblib.load('C:\\\\Users\\\\HP\\\\Desktop\\\\Mahidi\\\\FYP\\\\My Work\\\\Fashion Product Caption Generator\\\\Fashion Category Extractor\\\\num_listGender.pkl')\n",
    "    num_list_colour = joblib.load('C:\\\\Users\\\\HP\\\\Desktop\\\\Mahidi\\\\FYP\\\\My Work\\\\Fashion Product Caption Generator\\\\Fashion Category Extractor\\\\num_listColour.pkl')\n",
    "\n",
    "    # get the keys and values of each label dictionary\n",
    "    gender_keys = list(num_list_gender.keys())\n",
    "    gender_values = list(num_list_gender.values())\n",
    "    colour_keys = list(num_list_colour.keys())\n",
    "    colour_values = list(num_list_colour.values())\n",
    "    final_labels = []\n",
    "\n",
    "    # append the labels by mapping the index position to the values \n",
    "    final_labels.append(gender_keys[gender_values.index(out_label_1)])\n",
    "    final_labels.append(colour_keys[colour_values.index(out_label_2)])\n",
    "\n",
    "\n",
    "    # Convert the image from BGR to RGB\n",
    "    image_rgb = cv2.cvtColor(cv2.imread(image_p2), cv2.COLOR_BGR2RGB)\n",
    "    return final_labels[0] + \" \" + final_labels[1]\n",
    "    print(final_labels[0], final_labels[1])\n",
    "    \n",
    "image_p2 = \"C:\\\\Users\\\\HP\\\\Desktop\\\\Mahidi\\\\Pictures\\\\12.jpg\"\n",
    "category(image_p2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07194498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
