{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d27ee7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pretrainedmodels\n",
    "import cv2\n",
    "import torch.nn.functional as F\n",
    "import joblib\n",
    "\n",
    "'''MODEL_PATH = \"C:\\\\Users\\\\HP\\\\Desktop\\\\Mahidi\\\\FYP\\\\My Work\\\\Fashion Product Caption Generator\\\\fashion-ai-main\\\\fashion-ai-main\\\\models\\\\atr-recognition-stage-2-resnet34.pkl\"\n",
    "CLASSES_PATH = \"C:\\\\Users\\\\HP\\\\Desktop\\\\Mahidi\\\\FYP\\\\My Work\\\\Fashion Product Caption Generator\\\\fashion-ai-main\\\\fashion-ai-main\\\\clothes-categories\\\\attribute-classes.txt\"\n",
    "'''\n",
    "class ClassificationModel():\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "        \n",
    "    def load(self, model_path, labels_path,  eval=False):\n",
    "        self.model = torch.load(model_path)\n",
    "        self.model = nn.Sequential(self.model)\n",
    "        \n",
    "        self.labels = open(labels_path, 'r').read().splitlines()\n",
    "        \n",
    "        if eval:\n",
    "            print(model.eval())\n",
    "        return\n",
    "    \n",
    "    def predict(self, image_path):\n",
    "        \n",
    "        device = torch.device(\"cpu\")\n",
    "        img = Image.open(image_path)\n",
    "        \n",
    "        test_transforms = transforms.Compose([transforms.Resize(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                           [0.229, 0.224, 0.225])\n",
    "                                     ])\n",
    "        \n",
    "        image_tensor = test_transforms(img).float()\n",
    "        image_tensor = image_tensor.unsqueeze_(0)\n",
    "        inp = Variable(image_tensor)\n",
    "        inp = inp.to(device)\n",
    "        output = self.model(inp)\n",
    "        probabilities = output.data.cpu().numpy()\n",
    "        attribute_indices = np.argsort(probabilities)[0][::-1]\n",
    "        predicted_attributes = [self.labels[i] for i in attribute_indices]\n",
    "        first_two_attributes = predicted_attributes[:2]\n",
    "        attributes_str = ' '.join(first_two_attributes)\n",
    "        return attributes_str\n",
    "\n",
    "        '''print(first_two_attributes)'''\n",
    "        \n",
    "        \n",
    "    def predict1(self, image_path):\n",
    "        \n",
    "        device = torch.device(\"cpu\")\n",
    "        img = Image.open(image_path)\n",
    "        \n",
    "        test_transforms = transforms.Compose([transforms.Resize(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                           [0.229, 0.224, 0.225])\n",
    "                                     ])\n",
    "        \n",
    "        image_tensor = test_transforms(img).float()\n",
    "        image_tensor = image_tensor.unsqueeze_(0)\n",
    "        inp = Variable(image_tensor)\n",
    "        inp = inp.to(device)\n",
    "        output = self.model(inp)\n",
    "        index = output.data.cpu().numpy().argmax()\n",
    "        return self.labels[index]\n",
    "        \n",
    "        \n",
    "\n",
    "def caption(image_p):\n",
    "    '''\n",
    "    image_p = \"C:\\\\Users\\\\HP\\\\Desktop\\\\Mahidi\\\\FYP\\\\My Work\\\\Fashion Product Caption Generator\\\\fashion-ai-main\\\\fashion-ai-main\\\\clothes-categories\\\\img\\\\img\\\\Velveteen_Floral_Dress\\\\img_00000013.jpg\"\n",
    "    '''\n",
    "    learner = ClassificationModel()\n",
    "    learner.load(\"C:\\\\Users\\\\HP\\\\Desktop\\\\Mahidi\\\\FYP\\\\My Work\\\\Fashion Product Caption Generator\\\\fashion-ai-main\\\\fashion-ai-main\\\\models\\\\atr-recognition-stage-2-resnet34.pkl\", \"C:\\\\Users\\\\HP\\\\Desktop\\\\Mahidi\\\\FYP\\\\My Work\\\\Fashion Product Caption Generator\\\\fashion-ai-main\\\\fashion-ai-main\\\\clothes-categories\\\\attribute-classes.txt\")\n",
    "    prediction = learner.predict(image_p)\n",
    "    learner1 = ClassificationModel()\n",
    "    learner1.load(\"C:\\\\Users\\\\HP\\\\Desktop\\\\Mahidi\\\\FYP\\\\My Work\\\\Fashion Product Caption Generator\\\\fashion-ai-main\\\\fashion-ai-main\\\\cloth_cat_models\\\\stage-1_resnet34.pkl\", \"C:\\\\Users\\\\HP\\\\Desktop\\\\Mahidi\\\\FYP\\\\My Work\\\\Fashion Product Caption Generator\\\\fashion-ai-main\\\\fashion-ai-main\\\\clothes-categories\\\\classes.txt\")\n",
    "    prediction1 = learner1.predict1(image_p)\n",
    "    return prediction + \" \" + prediction1\n",
    "    '''print(prediction, prediction1)'''\n",
    "    \n",
    "'''caption()'''\n",
    "    \n",
    "'''def caption1():\n",
    "    learner = ClassificationModel()\n",
    "    learner.load(\"C:\\\\Users\\\\HP\\\\Desktop\\\\Mahidi\\\\FYP\\\\My Work\\\\Fashion Product Caption Generator\\\\fashion-ai-main\\\\fashion-ai-main\\\\\\cloth_cat_models\\\\stage-1_resnet34.pkl\", \"C:\\\\Users\\\\HP\\\\Desktop\\\\Mahidi\\\\FYP\\\\My Work\\\\Fashion Product Caption Generator\\\\fashion-ai-main\\\\fashion-ai-main\\\\clothes-categories\\\\classes.txt\")\n",
    "    prediction = learner.predict1(\"C:\\\\Users\\\\HP\\\\Desktop\\\\Mahidi\\\\FYP\\\\My Work\\\\Fashion Product Caption Generator\\\\fashion-ai-main\\\\fashion-ai-main\\\\clothes-categories\\\\img\\\\img\\\\Velveteen_Floral_Dress\\\\img_00000013.jpg\")\n",
    "    print(\"Predicted result:\", prediction)'''\n",
    "    \n",
    "\n",
    "'''//////////////////////////////////////////////////////////////////////////////////////////////////////////////'''\n",
    "class MultiHeadResNet50(nn.Module):\n",
    "    def __init__(self, pretrained, requires_grad):\n",
    "        super(MultiHeadResNet50, self).__init__()\n",
    "        if pretrained == True:\n",
    "            self.model = pretrainedmodels.__dict__['resnet50'](pretrained='imagenet')\n",
    "        else:\n",
    "            self.model = pretrainedmodels.__dict__['resnet50'](pretrained=None)\n",
    "        if requires_grad == True:\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = True\n",
    "            print('Training intermediate layer parameters...')\n",
    "        elif requires_grad == False:\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "            print('Freezing intermediate layer parameters...')\n",
    "        # change the final layers according to the number of categories\n",
    "        self.l0 = nn.Linear(2048, 5) # for gender\n",
    "        self.l1 = nn.Linear(2048, 48) # for baseColour\n",
    "    def forward(self, x):\n",
    "        # get the batch size only, ignore (c, h, w)\n",
    "        batch, _, _, _ = x.shape\n",
    "        x = self.model.features(x)\n",
    "        x = F.adaptive_avg_pool2d(x, 1).reshape(batch, -1)\n",
    "        l0 = self.l0(x)\n",
    "        l1 = self.l1(x)\n",
    "        return l0, l1\n",
    "\n",
    "\n",
    "def category(image_p2):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    \n",
    "    model = MultiHeadResNet50(pretrained=False, requires_grad=False)\n",
    "    checkpoint = torch.load('C:\\\\Users\\\\HP\\\\Desktop\\\\Mahidi\\\\FYP\\\\My Work\\\\Fashion Product Caption Generator\\\\Fashion Category Extractor\\\\category_extraction_model.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # read an image\n",
    "    image = cv2.imread(image_p2)\n",
    "    # keep a copy of the original image for OpenCV functions\n",
    "    orig_image = image.copy()\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # apply image transforms\n",
    "    image = transform(image)\n",
    "    # add batch dimension\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    # forward pass the image through the model\n",
    "    outputs = model(image)\n",
    "    # extract the two output\n",
    "    output1, output2 = outputs\n",
    "    # get the index positions of the highest label score\n",
    "    out_label_1 = np.argmax(output1.detach().cpu())\n",
    "    out_label_2 = np.argmax(output2.detach().cpu())\n",
    "\n",
    "    # load the label dictionaries\n",
    "    num_list_gender = joblib.load('C:\\\\Users\\\\HP\\\\Desktop\\\\Mahidi\\\\FYP\\\\My Work\\\\Fashion Product Caption Generator\\\\Fashion Category Extractor\\\\num_listGender.pkl')\n",
    "    num_list_colour = joblib.load('C:\\\\Users\\\\HP\\\\Desktop\\\\Mahidi\\\\FYP\\\\My Work\\\\Fashion Product Caption Generator\\\\Fashion Category Extractor\\\\num_listColour.pkl')\n",
    "\n",
    "    # get the keys and values of each label dictionary\n",
    "    gender_keys = list(num_list_gender.keys())\n",
    "    gender_values = list(num_list_gender.values())\n",
    "    colour_keys = list(num_list_colour.keys())\n",
    "    colour_values = list(num_list_colour.values())\n",
    "    final_labels = []\n",
    "\n",
    "    # append the labels by mapping the index position to the values \n",
    "    final_labels.append(gender_keys[gender_values.index(out_label_1)])\n",
    "    final_labels.append(colour_keys[colour_values.index(out_label_2)])\n",
    "\n",
    "\n",
    "    # Convert the image from BGR to RGB\n",
    "    image_rgb = cv2.cvtColor(cv2.imread(image_p2), cv2.COLOR_BGR2RGB)\n",
    "    return final_labels[0] + \" \" + final_labels[1]\n",
    "    '''print(final_labels[0], final_labels[1])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4087a08a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
